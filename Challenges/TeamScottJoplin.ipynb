{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Key estimation challenge\n",
    "\n",
    "This challenge was about finding out which key a piece was written in. The input was a set of .midi files containing performed pieces, and the output was a file that assigned each piece a key.\n",
    "\n",
    "To solve this problem, a recurrent neural network (RNN) was used as described by Liu et al. in [\"PERFORMANCE MIDI-TO-SCORE CONVERSION BY\n",
    "NEURAL BEAT TRACKING\"](https://www.turing.ac.uk/sites/default/files/2022-09/midi_quantisation_paper_ismir_2022_0.pdf). This paper is also explained in a [YouTube-Video](https://www.youtube.com/watch?v=yumxXCYSgbY) and their code is openly available at [GitHub](https://github.com/cheriell/PM2S). The architecture of the paper's neural networks is shown below. For our purposes, the branches ending with key signature and time signature numerators were implemented, but with more classes to support all 24 keys and 2 more numerators (9 and 12).\n",
    "\n",
    "![Architecture of the paper's neural networks.](img/NN.png)\n",
    "\n",
    "We altered their code base to work with the data set given to us and to produce output in the correct format.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9c6872a2e122ee9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Data\n",
    "\n",
    "At first, we have to load in our data in the format that is accepted by the RNN. Every piece should be transformed into a list of notes represented of a tuple (pitch, onset_sec, duration_sec, velocity). As the RNN is able to handle key signature changes, the label is not a single key, but a list of key signature changes. In our case there is always just one key signature change at the beginning of the piece."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7623616d29d11353"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Before we create the data set class, we have to take a look at data augmentation.\n",
    "\n",
    "An easy way to upscale the size of the given data set to prevent overfitting is augmenting the data. In this case, the tempo of the piece can be changed by scaling the onset and duration of every note by a given factor. Even more importantly, the piece can be transposed by changing the pitch of all notes simultaneously. When doing this, the key label also has to be changed, because a piece in B major that is transposed by a semitone upwards is now in C major. Please note that in the original code, there was an error that caused keys to be shifted incorrectly, causing the label of a piece in B major to be set to C minor instead of C major when pitching up by a semitone."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "436509bddd2d1a53"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class DataAugmentation:\n",
    "    def __init__(self,\n",
    "                 tempo_change_prob=1.0,\n",
    "                 tempo_change_range=(0.8, 1.2),\n",
    "                 pitch_shift_prob=1.0,\n",
    "                 pitch_shift_range=(-6, 6)):\n",
    "\n",
    "        self.tempo_change_prob = tempo_change_prob\n",
    "        self.tempo_change_range = tempo_change_range\n",
    "        self.pitch_shift_prob = pitch_shift_prob\n",
    "        self.pitch_shift_range = pitch_shift_range\n",
    "\n",
    "    def __call__(self, note_sequence, annotations):\n",
    "        # tempo change\n",
    "        if random.random() < self.tempo_change_prob:\n",
    "            note_sequence, annotations = self.tempo_change(note_sequence, annotations)\n",
    "\n",
    "        # pitch shift\n",
    "        if random.random() < self.pitch_shift_prob:\n",
    "            note_sequence, annotations = self.pitch_shift(note_sequence, annotations)\n",
    "\n",
    "        return note_sequence, annotations\n",
    "\n",
    "    def tempo_change(self, note_sequence, annotations):\n",
    "        tempo_change_ratio = random.uniform(*self.tempo_change_range)\n",
    "        note_sequence[:, 1:3] *= 1 / tempo_change_ratio\n",
    "        annotations['time_signatures'][:, 0] *= 1 / tempo_change_ratio\n",
    "        annotations['key_signatures'][:, 0] *= 1 / tempo_change_ratio\n",
    "        return note_sequence, annotations\n",
    "\n",
    "    def pitch_shift(self, note_sequence, annotations):\n",
    "        shift = round(random.uniform(*self.pitch_shift_range))\n",
    "        note_sequence[:, 0] += shift\n",
    "\n",
    "        for i in range(len(annotations['key_signatures'])):\n",
    "            key = annotations['key_signatures'][i, 1]\n",
    "            minor_offset = 12 * (key // 12)\n",
    "            scale_offset = key - minor_offset\n",
    "\n",
    "            annotations['key_signatures'][i, 1] = minor_offset + (scale_offset + shift) % 12\n",
    "\n",
    "        return note_sequence, annotations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:47.872689900Z",
     "start_time": "2023-12-27T14:00:47.846758800Z"
    }
   },
   "id": "2de1ecd0a77543f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Base Data Set\n",
    "\n",
    "As we use the same RNN for the key and meter estimation challenges, we have an abstract BaseDataSet for both of them. This data set holds the data and can then generate random batches to feed to the RNN for training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddbc5141ea2f327c"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.insert(0, os.path.join(sys.path[0], '..'))\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from pathlib import Path\n",
    "import partitura as pt\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, workspace, split):\n",
    "\n",
    "        # parameters\n",
    "        self.workspace = workspace\n",
    "        self.feature_folder = os.path.join(workspace, 'Dataset')\n",
    "        self.split = split\n",
    "\n",
    "        # Get metadata by split\n",
    "        self.metadata = pd.read_csv(os.path.join(self.feature_folder, 'key-meter_train_gt.txt'), delimiter=',')\n",
    "        self.metadata.reset_index(inplace=True)\n",
    "\n",
    "        # Get distinct pieces\n",
    "        self.piece2row = defaultdict(list)\n",
    "        for i, row in self.metadata.iterrows():\n",
    "            self.piece2row[row['filename']].append(i)\n",
    "        self.pieces = list(self.piece2row.keys())\n",
    "\n",
    "        # Initialise data augmentation\n",
    "        self.dataaug = DataAugmentation()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == 'train' or self.split == 'all':\n",
    "            # constantly update 200 steps per epoch, not related to training dataset size\n",
    "            return batch_size * 200\n",
    "\n",
    "        elif self.split == 'valid':\n",
    "            # by istinct pieces in validation set\n",
    "            return batch_size * len(self.piece2row) // 10  # valid dataset size\n",
    "\n",
    "        elif self.split == 'test':\n",
    "            return len(self.metadata)\n",
    "\n",
    "    def _sample_row(self, idx):\n",
    "        # Sample one row from the metadata\n",
    "        if self.split == 'train' or self.split == 'all':\n",
    "            piece_id = random.choice(list(self.piece2row.keys()))   # random sampling by piece\n",
    "            row_id = random.choice(self.piece2row[piece_id])\n",
    "        elif self.split == 'valid':\n",
    "            piece_id = self.pieces[idx // batch_size]    # by istinct pieces in validation set\n",
    "            row_id = self.piece2row[piece_id][idx % batch_size % len(self.piece2row[piece_id])]\n",
    "        elif self.split == 'test':\n",
    "            row_id = idx\n",
    "        row = self.metadata.iloc[row_id]\n",
    "\n",
    "        return row\n",
    "\n",
    "    def _load_data(self, row):\n",
    "        # Get feature\n",
    "        p = pt.load_performance_midi(str(Path(self.feature_folder, row['filename'])))\n",
    "        note_array = p.note_array()\n",
    "        note_sequence = np.array(list(zip(note_array['pitch'], note_array['onset_sec'], note_array['duration_sec'], note_array['velocity'])))\n",
    "        annotations = {\n",
    "            'time_signatures': np.array([(0., row['ts_num'])]),\n",
    "            'key_signatures': np.array([(0., keyName2Number[row['key']])]),\n",
    "            'tempo': np.array([(0., row['tempo'])])\n",
    "        }\n",
    "\n",
    "        # Data augmentation\n",
    "        if self.split == 'train' or self.split == 'all':\n",
    "            note_sequence, annotations = self.dataaug(note_sequence, annotations)\n",
    "\n",
    "        # Randomly sample a segment that is at most max_length long\n",
    "        if self.split == 'train' or self.split == 'all':\n",
    "            start_idx = random.randint(0, len(note_sequence)-1)\n",
    "            end_idx = start_idx + max_length\n",
    "        elif self.split == 'valid':\n",
    "            start_idx, end_idx = 0, max_length  # validate on the segment starting with the first note\n",
    "        elif self.split == 'test':\n",
    "            start_idx, end_idx = 0, len(note_sequence)  # test on the whole note sequence\n",
    "\n",
    "        if end_idx > len(note_sequence):\n",
    "            end_idx = len(note_sequence)\n",
    "\n",
    "        note_sequence = note_sequence[start_idx:end_idx]\n",
    "\n",
    "        return note_sequence, annotations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:48.202684900Z",
     "start_time": "2023-12-27T14:00:48.173761800Z"
    }
   },
   "id": "3fcddc7e7db9e2f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Key Signature Data Set\n",
    "\n",
    "To provide key estimation specific data, the abstract BaseDataSet is derived to return the correct label for every item in the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2c8e33ee096ad8d"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from PM2S.constants import *\n",
    "\n",
    "\n",
    "class KeySignatureDataset(BaseDataset):\n",
    "\n",
    "    def __init__(self, workspace, split):\n",
    "        super().__init__(workspace, split)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self._sample_row(idx)\n",
    "        note_sequence, annotations = self._load_data(row)\n",
    "\n",
    "        # Get model output data\n",
    "        key_signatures = annotations['key_signatures']\n",
    "\n",
    "        key_numbers = np.zeros(len(note_sequence)).astype(float)\n",
    "\n",
    "        for i in range(len(note_sequence)):\n",
    "            onset = note_sequence[i,1]\n",
    "            for ks in key_signatures:\n",
    "                if ks[0] > onset + tolerance:\n",
    "                    break\n",
    "                key_numbers[i] = ks[1] % keyVocabSize\n",
    "\n",
    "        # padding\n",
    "        length = len(note_sequence)\n",
    "        if length < max_length:\n",
    "            note_sequence = np.concatenate([note_sequence, np.zeros((max_length - length, 4))])\n",
    "            key_numbers = np.concatenate([key_numbers, np.zeros(max_length - length)])\n",
    "\n",
    "        return note_sequence, key_numbers, length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:48.462236100Z",
     "start_time": "2023-12-27T14:00:48.440260800Z"
    }
   },
   "id": "5fadc9217034dd3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data module\n",
    "Pytorch Lightning uses an implementation of a LightningDataModule for the training. It is needed for loading data differently during training, evaluating and testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c10e980cd41c414"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from PM2S.configs import *\n",
    "\n",
    "class Pm2sDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, args, feature='key_signature', full_train=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parameters from input arguments\n",
    "        self.workspace = args.workspace\n",
    "        self.feature = feature\n",
    "        self.full_train = full_train\n",
    "\n",
    "    def _get_dataset(self, split):\n",
    "        if self.feature == 'key_signature':\n",
    "            dataset = KeySignatureDataset(self.workspace, split)\n",
    "        elif self.feature == 'time_signature':\n",
    "            dataset = TimeSignatureDataset(self.workspace, split) # will be important later\n",
    "        else:\n",
    "            raise ValueError('Unknown feature: {}'.format(self.feature))\n",
    "        return dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.full_train:\n",
    "            dataset = self._get_dataset(split='all')\n",
    "        else:\n",
    "            dataset = self._get_dataset(split='train')\n",
    "        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
    "        dataloader = torch.utils.data.dataloader.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = self._get_dataset(split='valid')\n",
    "        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n",
    "        dataloader = torch.utils.data.dataloader.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=True\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = self._get_dataset(split='test')\n",
    "        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n",
    "        dataloader = torch.utils.data.dataloader.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=1,\n",
    "            sampler=sampler,\n",
    "            num_workers=0,\n",
    "            drop_last=False\n",
    "        )\n",
    "        return dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:48.699928200Z",
     "start_time": "2023-12-27T14:00:48.670647800Z"
    }
   },
   "id": "85630df345ad8dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This concludes loading the data, now we can generate our RNN and train it!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbae252b7ad053"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recurrent Neural Network\n",
    "\n",
    "The architecture of the RNN we use consumes a list of notes represented by tuples that gets fed through a convolutional neural network (CNN) block, a gated recurrent unit (GRU) block and then through a linear block with a dropout layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca54b209d4d1ef89"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from PM2S.models.blocks import ConvBlock, GRUBlock, LinearOutput\n",
    "from PM2S.constants import keyVocabSize\n",
    "from PM2S.models.utils import get_in_features, encode_note_sequence\n",
    "\n",
    "\n",
    "class RNNKeySignatureModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=512):\n",
    "        super().__init__()\n",
    "\n",
    "        in_features = get_in_features()\n",
    "\n",
    "        self.convs = ConvBlock(in_features=in_features)\n",
    "\n",
    "        self.gru = GRUBlock(in_features=hidden_size)\n",
    "\n",
    "        self.out = LinearOutput(in_features=hidden_size, out_features=keyVocabSize, activation_type='softmax')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, len(features)==4)\n",
    "        x = encode_note_sequence(x)\n",
    "\n",
    "        x = self.convs(x) # (batch_size, seq_len, hidden_size)\n",
    "        x = self.gru(x) # (batch_size, seq_len, hidden_size)\n",
    "        y = self.out(x) # (batch_size, seq_len, keyVocabSize)\n",
    "        y = y.transpose(1, 2) # (batch_size, keyVocabSize, seq_len)\n",
    "\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:49.066351900Z",
     "start_time": "2023-12-27T14:00:49.029450500Z"
    }
   },
   "id": "d2138fb6b9aa1ff5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Module\n",
    "\n",
    "Before we can start training, we have to wrap our model in a KeySignatureModel that handles training and validation steps by calculating the loss and f1 score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6452a59d5913044"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import os, sys\n",
    "\n",
    "from PM2S.modules.utils import configure_callbacks, configure_optimizers, classification_report_framewise\n",
    "\n",
    "sys.path.insert(0, os.path.join(sys.path[0], '..'))\n",
    "import torch.nn as nn\n",
    "\n",
    "class KeySignatureModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = RNNKeySignatureModel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return configure_optimizers(self)\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        return configure_callbacks(monitor='val_f1')\n",
    "\n",
    "    def training_step(self, batch, batch_size):\n",
    "        # Data\n",
    "        x, y, length = batch\n",
    "        x = x.float()\n",
    "        y = y.long()\n",
    "        length = length.long()\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # Mask out the padding part\n",
    "        mask = torch.ones(y_hat.shape).to(y_hat.device)\n",
    "        for i in range(y_hat.shape[0]):\n",
    "            mask[i, length[i]:] = 0\n",
    "        y_hat = y_hat * mask\n",
    "\n",
    "        # Loss\n",
    "        loss = nn.NLLLoss()(y_hat, y)\n",
    "\n",
    "        # Logging\n",
    "        logs = {\n",
    "            'train_loss': loss,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "\n",
    "        return {'loss': loss, 'logs': logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_size):\n",
    "        # Data\n",
    "        x, y, length = batch\n",
    "        x = x.float()\n",
    "        y = y.long()\n",
    "        length = length.long()\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # Mask out the padding part\n",
    "        for i in range(y_hat.shape[0]):\n",
    "            y_hat[i, length[i]:] = 0\n",
    "\n",
    "        # Loss\n",
    "        loss = nn.NLLLoss()(y_hat, y)\n",
    "\n",
    "        # Metrics\n",
    "        f_macro_all = 0\n",
    "\n",
    "        for i in range(y_hat.shape[0]):\n",
    "            # get sample from batch\n",
    "            y_hat_i = y_hat[i, :, :length[i]].topk(1, dim=0)[1][0]\n",
    "            y_i = y[i, :length[i]]\n",
    "\n",
    "            # get accuracies\n",
    "            (\n",
    "                _, _, f_macro,\n",
    "                _, _, _\n",
    "            ) = classification_report_framewise(y_i, y_hat_i)\n",
    "\n",
    "            f_macro_all += f_macro\n",
    "\n",
    "        f_macro_all /= y_hat.shape[0]\n",
    "\n",
    "        # Logging\n",
    "        logs = {\n",
    "            'val_loss': loss,\n",
    "            'val_f1': f_macro_all,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "\n",
    "        return {'loss': loss, 'logs': logs}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:49.324113200Z",
     "start_time": "2023-12-27T14:00:49.275194800Z"
    }
   },
   "id": "92efd1c3c770f2fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "Now, let's put everything together and train the model! If you have the whole dataset at hand, change the path in the code below to its location. Please make sure the first line has no '//' at the start because the column labels will not be processed then, also \"tempo(bpm)\" should be replaced by \"tempo\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff0f503660973fae"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # Data\n",
    "    data_module = Pm2sDataModule(args, feature=args.feature, full_train=args.full_train)\n",
    "\n",
    "    # Model\n",
    "    if args.feature == 'key_signature':\n",
    "        model = KeySignatureModule()\n",
    "    elif args.feature == 'time_signature':\n",
    "        model = TimeSignatureModule() # will be important later\n",
    "    else:\n",
    "        raise ValueError('Invalid feature type.')\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(args.workspace, 'mlruns'),\n",
    "        log_every_n_steps=50,\n",
    "        reload_dataloaders_every_n_epochs=True\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(model, data_module)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:49.674930800Z",
     "start_time": "2023-12-27T14:00:49.637998400Z"
    }
   },
   "id": "377bb7d6bd15ea56"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                 | Params\n",
      "-----------------------------------------------\n",
      "0 | model | RNNKeySignatureModel | 10.5 M\n",
      "-----------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.5 M    Total params\n",
      "42.011    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fd28efc8bb84b8bb659c9b6f941cf96"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\Dataset\\\\key-meter_train_gt.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m     feature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkey_signature\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      4\u001B[0m     full_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mKeyTrainArgs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[40], line 21\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m     14\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(\n\u001B[0;32m     15\u001B[0m     default_root_dir\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(args\u001B[38;5;241m.\u001B[39mworkspace, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmlruns\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m     16\u001B[0m     log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,\n\u001B[0;32m     17\u001B[0m     reload_dataloaders_every_n_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     18\u001B[0m )\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    576\u001B[0m     ckpt_path,\n\u001B[0;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    579\u001B[0m )\n\u001B[1;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:989\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    987\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    988\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 989\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    992\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    993\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    994\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1033\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[1;32m-> 1033\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1034\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m   1035\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1062\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1059\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1061\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m-> 1062\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1064\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1066\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:182\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    180\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[1;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loop_run(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:109\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;129m@_no_grad_context\u001B[39m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[_OUT_DICT]:\n\u001B[1;32m--> 109\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskip:\n\u001B[0;32m    111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:165\u001B[0m, in \u001B[0;36m_EvaluationLoop.setup_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    163\u001B[0m stage \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stage\n\u001B[0;32m    164\u001B[0m source \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_source\n\u001B[1;32m--> 165\u001B[0m dataloaders \u001B[38;5;241m=\u001B[39m \u001B[43m_request_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mbarrier(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstage\u001B[38;5;241m.\u001B[39mdataloader_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_dataloader()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataloaders, CombinedLoader):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:342\u001B[0m, in \u001B[0;36m_request_dataloader\u001B[1;34m(data_source)\u001B[0m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001B[39;00m\n\u001B[0;32m    332\u001B[0m \n\u001B[0;32m    333\u001B[0m \u001B[38;5;124;03mReturns:\u001B[39;00m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;124;03m    The requested dataloader\u001B[39;00m\n\u001B[0;32m    335\u001B[0m \n\u001B[0;32m    336\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _replace_dunder_methods(DataLoader, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m\"\u001B[39m), _replace_dunder_methods(BatchSampler):\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;66;03m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001B[39;00m\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;66;03m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001B[39;00m\n\u001B[0;32m    340\u001B[0m     \u001B[38;5;66;03m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001B[39;00m\n\u001B[0;32m    341\u001B[0m     \u001B[38;5;66;03m# methods so that the re-instantiated object is as close to the original as possible.\u001B[39;00m\n\u001B[1;32m--> 342\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdata_source\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:309\u001B[0m, in \u001B[0;36m_DataLoaderSource.dataloader\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstance, pl\u001B[38;5;241m.\u001B[39mLightningDataModule):\n\u001B[0;32m    308\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstance\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_datamodule_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minstance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstance \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstance\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:179\u001B[0m, in \u001B[0;36m_call_lightning_datamodule_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(fn):\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningDataModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mdatamodule\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[37], line 39\u001B[0m, in \u001B[0;36mPm2sDataModule.val_dataloader\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mval_dataloader\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 39\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m     sampler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39msampler\u001B[38;5;241m.\u001B[39mSequentialSampler(dataset)\n\u001B[0;32m     41\u001B[0m     dataloader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mdataloader\u001B[38;5;241m.\u001B[39mDataLoader(\n\u001B[0;32m     42\u001B[0m         dataset,\n\u001B[0;32m     43\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     46\u001B[0m         drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     47\u001B[0m     )\n",
      "Cell \u001B[1;32mIn[37], line 16\u001B[0m, in \u001B[0;36mPm2sDataModule._get_dataset\u001B[1;34m(self, split)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_dataset\u001B[39m(\u001B[38;5;28mself\u001B[39m, split):\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkey_signature\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 16\u001B[0m         dataset \u001B[38;5;241m=\u001B[39m \u001B[43mKeySignatureDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mworkspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_signature\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     18\u001B[0m         dataset \u001B[38;5;241m=\u001B[39m TimeSignatureDataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworkspace, split) \u001B[38;5;66;03m# will be important later\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[36], line 9\u001B[0m, in \u001B[0;36mKeySignatureDataset.__init__\u001B[1;34m(self, workspace, split)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, workspace, split):\n\u001B[1;32m----> 9\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mworkspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[35], line 21\u001B[0m, in \u001B[0;36mBaseDataset.__init__\u001B[1;34m(self, workspace, split)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit \u001B[38;5;241m=\u001B[39m split\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Get metadata by split\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkey-meter_train_gt.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mreset_index(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Get distinct pieces\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    900\u001B[0m     dialect,\n\u001B[0;32m    901\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    909\u001B[0m )\n\u001B[0;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    574\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    576\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 577\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1660\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1661\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1671\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1672\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\common.py:859\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    855\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    856\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    858\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '.\\\\Dataset\\\\key-meter_train_gt.txt'"
     ]
    }
   ],
   "source": [
    "class KeyTrainArgs:\n",
    "    workspace = '.' # set to your workspace which contains the dataset\n",
    "    feature = 'key_signature'\n",
    "    full_train = True\n",
    "\n",
    "train(KeyTrainArgs())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:51.459223800Z",
     "start_time": "2023-12-27T14:00:49.766170800Z"
    }
   },
   "id": "e55b2d40c49819b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model saving\n",
    "\n",
    "When the model is trained long enough, checkpoints are created in the 'mlruns' folder. To export these models for predicting the key of unlabelled pieces, we have to process and save them as a .pth file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "317cffde9c3872e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def save_model(args):\n",
    "    if args.feature == 'time_signature':\n",
    "        module = TimeSignatureModule.load_from_checkpoint(args.model_checkpoint_path) # will be important later\n",
    "        model_save_path = '../_model_state_dicts/time_signature/RNNTimeSignatureModel.pth'\n",
    "\n",
    "    elif args.feature == 'key_signature':\n",
    "        module = KeySignatureModule.load_from_checkpoint(args.model_checkpoint_path)\n",
    "        model_save_path = '../_model_state_dicts/key_signature/RNNKeySignatureModel.pth'\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Invalid feature type.')\n",
    "        \n",
    "    Path.mkdir(Path(model_save_path).parent, parents=True, exist_ok=True)\n",
    "    torch.save(module.model.state_dict(), model_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:51.461193100Z",
     "start_time": "2023-12-27T14:00:51.459223800Z"
    }
   },
   "id": "3ec0b02da846a967"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class KeySaveArgs:\n",
    "    model_checkpoint_path = os.path.join('.', 'mlruns', 'lightning_logs', 'version_#', 'checkpoints', '#.ckpt') # insert path your trained model here\n",
    "    feature = 'key_signature'\n",
    "\n",
    "save_model(KeySaveArgs())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:51.477184400Z",
     "start_time": "2023-12-27T14:00:51.463187600Z"
    }
   },
   "id": "7cc79124e4e5f883"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction\n",
    "\n",
    "The RNN we trained now can be fed data from .midi files to predict when the key of the piece changes. As our performances generally are only in a single key, we have to aggregate these outputs. To do so, we sum up all the durations of the predicted keys by subtracting their start time from the time of the next key change, or the end of the piece. Then, we take the key that has the longest duration in the piece."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca5da1f55fa1cf94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Processor\n",
    "\n",
    "We create a class that processes a collection of unlabelled inputs. Please download the trained model from out [GitHub](https://github.com/ZekReshi/Music-Informatics) by downloading the directory \"_model_state_dicts\" or set the path in the code below to your own model directory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "775b38aeaf594575"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from PM2S.features._processor import MIDIProcessor\n",
    "from PM2S.constants import keyNumber2Name\n",
    "\n",
    "\n",
    "class RNNKeySignatureProcessor(MIDIProcessor):\n",
    "\n",
    "    def __init__(self, model_state_dict_path='_model_state_dicts/key_signature/RNNKeySignatureModel.pth', **kwargs):\n",
    "        super().__init__(model_state_dict_path, **kwargs)\n",
    "\n",
    "    def load(self, state_dict_path):\n",
    "        if state_dict_path:\n",
    "            self._model = RNNKeySignatureModel()\n",
    "            self._model.load_state_dict(torch.load(state_dict_path))\n",
    "        else:\n",
    "            self._model = RNNKeySignatureModel()\n",
    "\n",
    "    def process(self, note_seq, **kwargs):\n",
    "        x = torch.tensor(note_seq).unsqueeze(0)\n",
    "\n",
    "        # Forward pass\n",
    "        key_probs = self._model(x)\n",
    "\n",
    "        # Post-processing\n",
    "        key_idx = key_probs[0].topk(1, dim=0)[1].squeeze(0).cpu().detach().numpy() # (seq_len,)\n",
    "\n",
    "        onsets = note_seq[:, 1]\n",
    "        key_signature_changes = self.pps(key_idx, onsets)\n",
    "\n",
    "        return key_signature_changes\n",
    "\n",
    "    def pps(self, key_idx, onsets):\n",
    "        ks_prev = '0'\n",
    "        ks_changes = []\n",
    "        for i in range(len(key_idx)):\n",
    "            ks_cur = keyNumber2Name[key_idx[i]]\n",
    "            if i == 0 or ks_cur != ks_prev:\n",
    "                onset_cur = onsets[i]\n",
    "                ks_changes.append((onset_cur, ks_cur))\n",
    "                ks_prev = ks_cur\n",
    "        return ks_changes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T14:00:51.466181500Z"
    }
   },
   "id": "8548ad37d48bcfa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class KeyPredictionArgs:\n",
    "    datadir = os.path.join('.', 'Dataset')\n",
    "    modeldir = '.' # insert path to our saved model here\n",
    "    \n",
    "args = KeyPredictionArgs()\n",
    "\n",
    "midi_files = glob.glob(os.path.join(args.datadir, \"*.mid\"))\n",
    "midi_files.sort()\n",
    "\n",
    "# Create time and key processors\n",
    "processor_key_sig = RNNKeySignatureProcessor(os.path.join(args.modeldir, '_model_state_dicts', 'key_signature', 'RNNKeySignatureModel.pth'))\n",
    "\n",
    "results = []\n",
    "# Prediction\n",
    "for idx, file in enumerate(midi_files):\n",
    "    p = pt.load_performance_midi(Path(file))\n",
    "    note_array = p.note_array()\n",
    "    note_sequence = np.array(list(zip(note_array['pitch'], note_array['onset_sec'], note_array['duration_sec'], note_array['velocity'])))\n",
    "\n",
    "    key_signature_changes = processor_key_sig.process(note_sequence)\n",
    "\n",
    "    length = note_sequence[-1][1] + note_sequence[-1][2]\n",
    "\n",
    "    last_onset, last_key = key_signature_changes[0]\n",
    "    durations = {}\n",
    "    for onset, key in key_signature_changes[1:]:\n",
    "        if last_key not in durations.keys():\n",
    "            durations[last_key] = 0\n",
    "        durations[last_key] += onset - last_onset\n",
    "        last_onset = onset\n",
    "        last_key = key\n",
    "    if last_key not in durations.keys():\n",
    "        durations[last_key] = 0\n",
    "    durations[last_key] += length - last_onset\n",
    "\n",
    "    best_duration, best_key = 0, 0\n",
    "    for key, duration in durations.items():\n",
    "        if duration > best_duration:\n",
    "            best_duration = duration\n",
    "            best_key = key\n",
    "\n",
    "    print(f\"{str(idx + 1)}/{str(len(midi_files))}: {file}\")\n",
    "    print(\"Prediction: \" + str(best_key))\n",
    "\n",
    "    results.append((os.path.basename(file), best_key))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T14:00:51.470171100Z"
    }
   },
   "id": "60f1f5c283fce51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "This model, when applied to our data, is able to solve this challenge in a satisfying manner as our team is placed number 1 on the leaderboard (as of December 26th). After a couple of hours of training (about 13 epochs), we already achieved an f-score of 0.97, but this score must be taken with a grain of salt as the verification is done with pieces from the training set. We reached an average tonal distance of 0.379, which tells us that our model is right most of the time, and if it does not output the right key it is probably not that far off, i.e. a C major key is wrongly detected as an A minor key, which uses the same notes but has a different root."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca316e65ae461da7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Meter estimation challenge\n",
    "\n",
    "Very similarly to the key estimation challenge, the meter estimation challenge is about processing performed pieces to calculate which meter the piece is written in, e.g. 4/4 or 3/4. But this challenge only takes the numerator into account because the challenge already is quite hard. The provided dataset is the same as the dataset for the key estimation challenge, and our solution also uses many components that have been seen in the above paragraphs. Therefore, it is required to execute the code above for this part to work. \n",
    "\n",
    "At first, we will only look at the meter, not the tempo."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986710b7b828b5d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "This part is nearly a carbon copy of the one above, so we will just create the classes without further elaboration."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ccf7c5e30a7fd50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TimeSignatureDataset(BaseDataset):\n",
    "\n",
    "    def __init__(self, workspace, split):\n",
    "        super().__init__(workspace, split)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self._sample_row(idx)\n",
    "        note_sequence, annotations = self._load_data(row)\n",
    "\n",
    "        # Get model output data\n",
    "        time_signatures = annotations['time_signatures']\n",
    "        ts_numerators = np.zeros(len(note_sequence)).astype(float)\n",
    "\n",
    "        for i in range(len(note_sequence)):\n",
    "            onset = note_sequence[i, 1]\n",
    "            for ts in time_signatures:\n",
    "                if ts[0] > onset + tolerance:\n",
    "                    break\n",
    "                ts_numerators[i] = tsNume2Index[int(ts[1])] if int(ts[1]) in tsNume2Index.keys() else 0\n",
    "\n",
    "        # padding\n",
    "        length = len(note_sequence)\n",
    "        if length < max_length:\n",
    "            note_sequence = np.concatenate([note_sequence, np.zeros((max_length - length, 4))])\n",
    "            ts_numerators = np.concatenate([ts_numerators, np.zeros(max_length - length)])\n",
    "\n",
    "        return note_sequence, ts_numerators, length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T14:00:51.472163Z"
    }
   },
   "id": "61d72ca5e55f52aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "The RNN model as well as the module wrapper is also the same as for the key estimation challenge, the only difference being the last layer, which of course has to provide different (less) classes as an output."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c909756b92cd0bf3"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class RNNTimeSignatureModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=512):\n",
    "        super().__init__()\n",
    "\n",
    "        in_features = get_in_features()\n",
    "\n",
    "        self.convs = ConvBlock(in_features=in_features)\n",
    "        self.gru = GRUBlock(in_features=hidden_size)\n",
    "        self.out_tn = LinearOutput(in_features=hidden_size, out_features=tsNumeVocabSize, activation_type='softmax')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, len(features)==4)\n",
    "        x = encode_note_sequence(x)\n",
    "\n",
    "        x = self.convs(x)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        x_gru = self.gru(x)  # (batch_size, seq_len, hidden_size)\n",
    "        y_tn = self.out_tn(x_gru)  # (batch_size, seq_len, tsNumeVocabSize)\n",
    "        y_tn = y_tn.permute(0, 2, 1)  # (batch_size, tsNumeVocabSize, seq_len)\n",
    "\n",
    "        return y_tn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:54.030228200Z",
     "start_time": "2023-12-27T14:00:54.003253300Z"
    }
   },
   "id": "70f6c0e5d25e95f4"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class TimeSignatureModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = RNNTimeSignatureModel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return configure_optimizers(self)\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        return configure_callbacks(monitor='val_f1')\n",
    "\n",
    "    def training_step(self, batch, batch_size):\n",
    "        # Data\n",
    "        x, y_tn, length = batch\n",
    "        x = x.float()\n",
    "        y_tn = y_tn.long()\n",
    "        length = length.long()\n",
    "\n",
    "        # Forward pass\n",
    "        y_tn_hat = self(x)\n",
    "\n",
    "        # Mask out the padding part\n",
    "        pad_mask = torch.ones((y_tn_hat.shape[0], y_tn_hat.shape[2])).to(y_tn_hat.device)\n",
    "        for i in range(y_tn_hat.shape[0]):\n",
    "            pad_mask[i, length[i]:] = 0\n",
    "        y_tn_hat = y_tn_hat * pad_mask.unsqueeze(1)\n",
    "\n",
    "        # Loss\n",
    "        loss = nn.NLLLoss(ignore_index=0)(y_tn_hat, y_tn)\n",
    "\n",
    "        # Logging\n",
    "        logs = {\n",
    "            'train_loss': loss,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "\n",
    "        return {'loss': loss, 'logs': logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_size):\n",
    "        # Data\n",
    "        x, y_tn, length = batch\n",
    "        x = x.float()\n",
    "        y_tn = y_tn.long()\n",
    "        length = length.long()\n",
    "\n",
    "        # Forward pass\n",
    "        y_tn_hat = self(x)\n",
    "\n",
    "        # Mask out the padding part\n",
    "        for i in range(y_tn_hat.shape[0]):\n",
    "            y_tn_hat[i, :, length[i]:] = 0\n",
    "\n",
    "        # Loss\n",
    "        loss = nn.NLLLoss(ignore_index=0)(y_tn_hat, y_tn)\n",
    "\n",
    "        # Metrics\n",
    "        fs_macro_tn = 0\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            # get sample from batch\n",
    "            y_tn_hat_i = y_tn_hat[i, :, :length[i]].topk(1, dim=0)[1][0]\n",
    "            y_tn_i = y_tn[i, :length[i]]\n",
    "\n",
    "            # filter out ignored indexes (the same as padding)\n",
    "            y_tn_hat_i = y_tn_hat_i[y_tn_i != 0]\n",
    "            y_tn_i = y_tn_i[y_tn_i != 0]\n",
    "\n",
    "            # get accuracies\n",
    "            (\n",
    "                _, _, f_macro_tn,\n",
    "                _, _, _\n",
    "            ) = classification_report_framewise(y_tn_i, y_tn_hat_i)\n",
    "\n",
    "            fs_macro_tn += f_macro_tn\n",
    "\n",
    "        fs_macro_tn /= x.shape[0]\n",
    "\n",
    "        # Logging\n",
    "        logs = {\n",
    "            'val_loss': loss,\n",
    "            'val_f1': fs_macro_tn,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "\n",
    "        return {'loss': loss, 'logs': logs}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:54.793175200Z",
     "start_time": "2023-12-27T14:00:54.745238800Z"
    }
   },
   "id": "4661321241b1fa9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "Now, let's train the model!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8c6bd22f39546c9"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | RNNTimeSignatureModel | 10.5 M\n",
      "------------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.5 M    Total params\n",
      "41.976    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26814c91bcd8441e8b2f7b7e5cca2276"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\Dataset\\\\key-meter_train_gt.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m     feature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_signature\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      4\u001B[0m     full_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMeterTrainArgs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[40], line 21\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m     14\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(\n\u001B[0;32m     15\u001B[0m     default_root_dir\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(args\u001B[38;5;241m.\u001B[39mworkspace, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmlruns\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m     16\u001B[0m     log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,\n\u001B[0;32m     17\u001B[0m     reload_dataloaders_every_n_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     18\u001B[0m )\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    576\u001B[0m     ckpt_path,\n\u001B[0;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    579\u001B[0m )\n\u001B[1;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:989\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    987\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    988\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 989\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    992\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    993\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    994\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1033\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[1;32m-> 1033\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1034\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m   1035\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1062\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1059\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1061\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m-> 1062\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1064\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1066\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:182\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    180\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[1;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loop_run(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:109\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;129m@_no_grad_context\u001B[39m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[_OUT_DICT]:\n\u001B[1;32m--> 109\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskip:\n\u001B[0;32m    111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:165\u001B[0m, in \u001B[0;36m_EvaluationLoop.setup_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    163\u001B[0m stage \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stage\n\u001B[0;32m    164\u001B[0m source \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_source\n\u001B[1;32m--> 165\u001B[0m dataloaders \u001B[38;5;241m=\u001B[39m \u001B[43m_request_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mbarrier(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstage\u001B[38;5;241m.\u001B[39mdataloader_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_dataloader()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataloaders, CombinedLoader):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:342\u001B[0m, in \u001B[0;36m_request_dataloader\u001B[1;34m(data_source)\u001B[0m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001B[39;00m\n\u001B[0;32m    332\u001B[0m \n\u001B[0;32m    333\u001B[0m \u001B[38;5;124;03mReturns:\u001B[39;00m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;124;03m    The requested dataloader\u001B[39;00m\n\u001B[0;32m    335\u001B[0m \n\u001B[0;32m    336\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _replace_dunder_methods(DataLoader, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m\"\u001B[39m), _replace_dunder_methods(BatchSampler):\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;66;03m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001B[39;00m\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;66;03m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001B[39;00m\n\u001B[0;32m    340\u001B[0m     \u001B[38;5;66;03m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001B[39;00m\n\u001B[0;32m    341\u001B[0m     \u001B[38;5;66;03m# methods so that the re-instantiated object is as close to the original as possible.\u001B[39;00m\n\u001B[1;32m--> 342\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdata_source\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:309\u001B[0m, in \u001B[0;36m_DataLoaderSource.dataloader\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstance, pl\u001B[38;5;241m.\u001B[39mLightningDataModule):\n\u001B[0;32m    308\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstance\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_datamodule_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minstance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstance \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstance\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:179\u001B[0m, in \u001B[0;36m_call_lightning_datamodule_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(fn):\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningDataModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mdatamodule\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[37], line 39\u001B[0m, in \u001B[0;36mPm2sDataModule.val_dataloader\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mval_dataloader\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 39\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m     sampler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39msampler\u001B[38;5;241m.\u001B[39mSequentialSampler(dataset)\n\u001B[0;32m     41\u001B[0m     dataloader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mdataloader\u001B[38;5;241m.\u001B[39mDataLoader(\n\u001B[0;32m     42\u001B[0m         dataset,\n\u001B[0;32m     43\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     46\u001B[0m         drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     47\u001B[0m     )\n",
      "Cell \u001B[1;32mIn[37], line 18\u001B[0m, in \u001B[0;36mPm2sDataModule._get_dataset\u001B[1;34m(self, split)\u001B[0m\n\u001B[0;32m     16\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m KeySignatureDataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworkspace, split)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_signature\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 18\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mTimeSignatureDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mworkspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# will be important later\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnknown feature: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature))\n",
      "Cell \u001B[1;32mIn[14], line 4\u001B[0m, in \u001B[0;36mTimeSignatureDataset.__init__\u001B[1;34m(self, workspace, split)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, workspace, split):\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mworkspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[35], line 21\u001B[0m, in \u001B[0;36mBaseDataset.__init__\u001B[1;34m(self, workspace, split)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit \u001B[38;5;241m=\u001B[39m split\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Get metadata by split\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkey-meter_train_gt.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mreset_index(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Get distinct pieces\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    900\u001B[0m     dialect,\n\u001B[0;32m    901\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    909\u001B[0m )\n\u001B[0;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    574\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    576\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 577\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1660\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1661\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1671\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1672\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\pandas\\io\\common.py:859\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    855\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    856\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    858\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '.\\\\Dataset\\\\key-meter_train_gt.txt'"
     ]
    }
   ],
   "source": [
    "class MeterTrainArgs:\n",
    "    workspace = '.' # set to your workspace which contains the dataset\n",
    "    feature = 'time_signature'\n",
    "    full_train = True\n",
    "\n",
    "train(MeterTrainArgs())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:56.308557700Z",
     "start_time": "2023-12-27T14:00:55.729118100Z"
    }
   },
   "id": "2dacf1ad364452a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model saving\n",
    "Now, let's save the model again."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc96b911c66fb5a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MeterSaveArgs:\n",
    "    model_checkpoint_path = os.path.join('.', 'mlruns', 'lightning_logs', 'version_#', 'checkpoints', '#.ckpt') # insert path your trained model here\n",
    "    feature = 'time_signature'\n",
    "\n",
    "save_model(MeterSaveArgs())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T14:00:56.299588400Z"
    }
   },
   "id": "cc86980f8e5bd585"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction\n",
    "Yet again, we employ the same technique as we did when solving the key estimation challenge. The RNN outputs a list of time signature changes, and we aggregate them to find the time signature that is predicted for the longest duration."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8ced3253a09191c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNNTimeSignatureProcessor(MIDIProcessor):\n",
    "\n",
    "    def __init__(self, model_state_dict_path='_model_state_dicts/time_signature/RNNTimeSignatureModel.pth', **kwargs):\n",
    "        super().__init__(model_state_dict_path, **kwargs)\n",
    "\n",
    "    def load(self, state_dict_path):\n",
    "        if state_dict_path:\n",
    "            self._model = RNNTimeSignatureModel()\n",
    "            self._model.load_state_dict(torch.load(state_dict_path))\n",
    "        else:\n",
    "            self._model = RNNTimeSignatureModel()\n",
    "\n",
    "    def process(self, note_seq, **kwargs):\n",
    "        x = torch.tensor(note_seq).unsqueeze(0)\n",
    "\n",
    "        # Forward pass\n",
    "        tn_probs = self._model(x)\n",
    "\n",
    "        # Post-processing\n",
    "        tn_idx = tn_probs[0].topk(1, dim=0)[1].squeeze(0).cpu().detach().numpy() # (seq_len,)\n",
    "\n",
    "        onsets = note_seq[:, 1]\n",
    "        time_signature_changes = self.pps(tn_idx, onsets)\n",
    "\n",
    "        return time_signature_changes\n",
    "\n",
    "    def pps(self, tn_idx, onsets):\n",
    "        ts_prev = '0/0'\n",
    "        ts_changes = []\n",
    "        for i in range(len(tn_idx)):\n",
    "            ts_cur = '{:d}'.format(tsIndex2Nume[tn_idx[i]])\n",
    "            if i == 0 or ts_cur != ts_prev:\n",
    "                onset_cur = onsets[i]\n",
    "                ts_changes.append((onset_cur, ts_cur))\n",
    "                ts_prev = ts_cur\n",
    "        return ts_changes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T14:00:56.301576500Z"
    }
   },
   "id": "d0672c37b2e760d0"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\_model_state_dicts\\\\time_signature\\\\RNNTimeSignatureModel.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m midi_files\u001B[38;5;241m.\u001B[39msort()\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Create time and key processors\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m processor_time_sig \u001B[38;5;241m=\u001B[39m \u001B[43mRNNTimeSignatureProcessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodeldir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_model_state_dicts\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtime_signature\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRNNTimeSignatureModel.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Prediction\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, file \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(midi_files):\n",
      "Cell \u001B[1;32mIn[19], line 4\u001B[0m, in \u001B[0;36mRNNTimeSignatureProcessor.__init__\u001B[1;34m(self, model_state_dict_path, **kwargs)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model_state_dict_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_model_state_dicts/time_signature/RNNTimeSignatureModel.pth\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(model_state_dict_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Documents\\Uni\\MusicInformatics\\Music-Informatics\\Challenges\\PM2S\\features\\_processor.py:16\u001B[0m, in \u001B[0;36mMIDIProcessor.__init__\u001B[1;34m(self, model_state_dict_path, **kwargs)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m----------\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03mkwargs : dict\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_state_dict_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39meval()\n",
      "Cell \u001B[1;32mIn[19], line 9\u001B[0m, in \u001B[0;36mRNNTimeSignatureProcessor.load\u001B[1;34m(self, state_dict_path)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m state_dict_path:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model \u001B[38;5;241m=\u001B[39m RNNTimeSignatureModel()\n\u001B[1;32m----> 9\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_dict_path\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model \u001B[38;5;241m=\u001B[39m RNNTimeSignatureModel()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\torch\\serialization.py:986\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    984\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 986\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    987\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    988\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    989\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    990\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    991\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\torch\\serialization.py:435\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    434\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 435\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    436\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    437\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\torch\\serialization.py:416\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 416\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '.\\\\_model_state_dicts\\\\time_signature\\\\RNNTimeSignatureModel.pth'"
     ]
    }
   ],
   "source": [
    "class MeterPredictionArgs:\n",
    "    datadir = os.path.join('.', 'Dataset')\n",
    "    modeldir = '.' # insert path to our saved model here\n",
    "    \n",
    "args = MeterPredictionArgs()\n",
    "\n",
    "midi_files = glob.glob(os.path.join(args.datadir, \"*.mid\"))\n",
    "midi_files.sort()\n",
    "\n",
    "# Create time and key processors\n",
    "processor_time_sig = RNNTimeSignatureProcessor(os.path.join(args.modeldir, '_model_state_dicts', 'time_signature', 'RNNTimeSignatureModel.pth'))\n",
    "\n",
    "# Prediction\n",
    "for idx, file in enumerate(midi_files):\n",
    "    p = pt.load_performance_midi(Path(file))\n",
    "    note_array = p.note_array()\n",
    "    note_sequence = np.array(list(zip(note_array['pitch'], note_array['onset_sec'], note_array['duration_sec'], note_array['velocity'])))\n",
    "\n",
    "    time_signature_changes = processor_time_sig.process(note_sequence)\n",
    "\n",
    "    length = note_sequence[-1][1] + note_sequence[-1][2]\n",
    "\n",
    "    last_onset, last_ts_num = time_signature_changes[0]\n",
    "    durations = {}\n",
    "    for onset, ts_num in time_signature_changes[1:]:\n",
    "        if last_ts_num not in durations.keys():\n",
    "            durations[last_ts_num] = 0\n",
    "        durations[last_ts_num] += onset - last_onset\n",
    "        last_onset = onset\n",
    "        last_ts_num = ts_num\n",
    "    if last_ts_num not in durations.keys():\n",
    "        durations[last_ts_num] = 0\n",
    "    durations[last_ts_num] += length - last_onset\n",
    "\n",
    "    best_duration, best_ts_num = 0, 0\n",
    "    for ts_num, duration in durations.items():\n",
    "        if duration > best_duration:\n",
    "            best_duration = duration\n",
    "            best_ts_num = ts_num\n",
    "\n",
    "    print(f\"{str(idx + 1)}/{str(len(midi_files))}: {file}\")\n",
    "    print(\"Prediction: \" + str(best_ts_num))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:56.694071300Z",
     "start_time": "2023-12-27T14:00:56.421799600Z"
    }
   },
   "id": "64daa6c5bb647d32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion of meter estimation\n",
    "\n",
    "With this model, we were able to achieve an f-score of about 0.47, but again this was done on some pieces of the training set. This might not seem like much, but when we applied it so the test set, we reached a score of 0.572, just below the current best meter estimation on the leaderboard (0.575 as of December 28th).\n",
    "\n",
    "Now, we have to take a look at the tempo estimation!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7c44f5232f077d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tempo\n",
    "\n",
    "To calculate the tempo, we make use of the hidden markov model (HMM) presented in the lecture with some slight alterations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9134a36c5e5e84a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Subbeats from durations\n",
    "\n",
    "Initially, the HMMs tried every other option with a sub beat count of 2 or 3, but we observed a pattern in many of the train pieces: Very often, a note is played for the duration of a beat, while other notes (mostly higher notes) are played every sub beat. So we could generate a histogram of note durations, search for peaks, and then see if the first two peaks have a factor close to 2 or 3. In this case, only that option is considered when trying out different configurations with the HMM."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1caf19f05bc5026"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def subbeats_from_durations(note_array: np.ndarray):\n",
    "    durations = note_array[\"duration_sec\"]\n",
    "    hist, bins = np.histogram(durations, bins=100)\n",
    "\n",
    "    peaks, _ = find_peaks(hist, prominence=20)\n",
    "\n",
    "    if len(peaks) > 1:\n",
    "        candidate = bins[peaks[1]] / bins[peaks[0]]\n",
    "        if 1.9 < candidate < 2.1:\n",
    "            return [2]\n",
    "        if 2.9 < candidate < 3.1:\n",
    "            return [3]\n",
    "    return [2, 3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:56.844820200Z",
     "start_time": "2023-12-27T14:00:56.809760600Z"
    }
   },
   "id": "6b02c516f2847d9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tempi from inter onset intervals\n",
    "\n",
    "Calculating the possible tempi by using autocorrelation causes many different tempi to be considered, and most of the time their values are too 'clean', like 120.0, 240.0 or 43.43434343. We try to use the histogram of inter onset intervals (IOIs) to detect how long a beat is by finding the first valid peak and calculating the beats per minute. Then, we calculate a couple of multipliers to add to the tempi list, precisely 1, 2, and 4 as the piece might have a lot of 16th notes although it is written in 4/4, for example."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f3056e6c2aed4e2"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def tempi_from_iois(note_array: np.ndarray, min_tempo: float, max_tempo: float):\n",
    "    IOIs = np.diff(np.sort(note_array[\"onset_sec\"]))\n",
    "    hist, bins = np.histogram(IOIs, bins=100)\n",
    "\n",
    "    valid_from = 0\n",
    "    for i in range(len(bins)):\n",
    "        if bins[i] >= 1 / 16:\n",
    "            valid_from = i\n",
    "            break\n",
    "\n",
    "    new_hist = []\n",
    "    new_labels = []\n",
    "    for i in range(valid_from - 1, len(hist) - 1):\n",
    "        new_hist.append((hist[i-1] + hist[i] + hist[i+1]) / 3)\n",
    "        new_labels.append((bins[i+1] + bins[i]) / 2)\n",
    "\n",
    "    peaks, _ = find_peaks(new_hist, prominence=5)\n",
    "\n",
    "    if len(peaks) == 0:\n",
    "        return []\n",
    "\n",
    "    beat_duration = new_labels[peaks[0]]\n",
    "    beats_per_minute = 60 / beat_duration\n",
    "    tempi = []\n",
    "    for i in range(3):\n",
    "        tempo = beats_per_minute / (2 ** i)\n",
    "        if min_tempo < tempo < max_tempo:\n",
    "            tempi.append(tempo)\n",
    "    return tempi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:58.420947Z",
     "start_time": "2023-12-27T14:00:58.386038400Z"
    }
   },
   "id": "dfe86d76e163e259"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hidden Markov Model\n",
    "\n",
    "Now, we just take the given HMM from the lecture which was provided via Baseline_Meter.py."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a53dd3c4f8642f4f"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterable\n",
    "\n",
    "from hiddenmarkov import ConstantTransitionModel, ObservationModel\n",
    "\n",
    "FRAMERATE = 16\n",
    "\n",
    "\n",
    "class MeterObservationModel(ObservationModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: int = 20,\n",
    "        downbeat_idx: Iterable = [0],\n",
    "        beat_idx: Iterable = [50],\n",
    "        subbeat_idx: Iterable = [25],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.states = states\n",
    "        # observation 1 = note onset present, 0 = nothing present\n",
    "        self.probabilities = np.ones((2, states)) / 100\n",
    "        self.probabilities[0, :] = 0.99\n",
    "        for idx in subbeat_idx:\n",
    "            self.probabilities[:, idx] = [0.5, 0.5]\n",
    "        for idx in beat_idx:\n",
    "            self.probabilities[:, idx] = [0.3, 0.7]\n",
    "        for idx in downbeat_idx:\n",
    "            self.probabilities[:, idx] = [0.1, 0.9]\n",
    "        self.db = downbeat_idx\n",
    "        self.b = beat_idx\n",
    "        self.sb = subbeat_idx\n",
    "\n",
    "    def get_beat_states(self, state_sequence: np.ndarray) -> np.ndarray:\n",
    "        state_encoder = np.zeros_like(state_sequence)\n",
    "        for i, state in enumerate(state_sequence):\n",
    "            if state in self.sb:\n",
    "                state_encoder[i] = 1\n",
    "            if state in self.b:\n",
    "                state_encoder[i] = 2\n",
    "            if state in self.db:\n",
    "                state_encoder[i] = 3\n",
    "        return state_encoder\n",
    "\n",
    "    def __call__(self, observation: np.ndarray) -> np.ndarray:\n",
    "        if not self.use_log_probabilities:\n",
    "            return self.probabilities[observation, :]\n",
    "        else:\n",
    "            return np.log(self.probabilities[observation, :])\n",
    "\n",
    "\n",
    "def getTransitionMatrix(\n",
    "    states: int,\n",
    "    distribution: Iterable = [0.1, 0.8, 0.1],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute transition Matrix\n",
    "    \"\"\"\n",
    "    if states == 1:\n",
    "        raise ValueError(\"The number of states should be > 1\")\n",
    "    transition_matrix = (\n",
    "        np.eye(states, k=0) * distribution[0]\n",
    "        + np.eye(states, k=1) * distribution[1]\n",
    "        + np.eye(states, k=2) * distribution[2]\n",
    "        + np.ones((states, states)) / 1e7\n",
    "    )\n",
    "\n",
    "    transition_matrix[-2, 0] = distribution[2]\n",
    "    transition_matrix[-1, 0] = distribution[2] + distribution[1]\n",
    "\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def createHMM(\n",
    "    tempo: float = 50,\n",
    "    frame_rate: int = FRAMERATE,  # frames_per_beat\n",
    "    beats_per_measure: int = 4,\n",
    "    subbeats_per_beat: int = 2,\n",
    ") -> Tuple[MeterObservationModel, ConstantTransitionModel]:\n",
    "    frames_per_beat = 60 / tempo * frame_rate\n",
    "    frames_per_measure = frames_per_beat * beats_per_measure\n",
    "    states = int(frames_per_measure)\n",
    "    downbeat_idx = [0]\n",
    "    beat_idx = [int(states / beats_per_measure * k) for k in range(beats_per_measure)]\n",
    "    subbeat_idx = [\n",
    "        int(states / (beats_per_measure * subbeats_per_beat) * k)\n",
    "        for k in range(beats_per_measure * subbeats_per_beat)\n",
    "    ]\n",
    "\n",
    "    observation_model = MeterObservationModel(\n",
    "        states=states,\n",
    "        downbeat_idx=downbeat_idx,\n",
    "        beat_idx=beat_idx,\n",
    "        subbeat_idx=subbeat_idx,\n",
    "    )\n",
    "\n",
    "    transition_matrix = getTransitionMatrix(states)\n",
    "    transition_model = ConstantTransitionModel(transition_matrix)\n",
    "\n",
    "    return observation_model, transition_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:00:59.448044400Z",
     "start_time": "2023-12-27T14:00:59.376235600Z"
    }
   },
   "id": "207a951430649f0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tempo estimation\n",
    "\n",
    "The method to finally estimate the tempo is nearly identical to the one we received initially, but it now makes use of the sub beat extraction from durations and the tempo candidates extracted from the IOIs. When there are no tempo candidates, we fall back to the autocorrelation again."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c906ba918738020"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from partitura.utils.misc import PathLike\n",
    "from meter_estimation_utils import get_frames_chordify, compute_autocorrelation\n",
    "from hiddenmarkov import HMM\n",
    "\n",
    "\n",
    "def estimate_tempo(\n",
    "    filename: PathLike,\n",
    "    beats_per_measure: int,\n",
    "    framerate: int = FRAMERATE,\n",
    "    frame_threshold: float = 0.0,\n",
    "    chord_spread_time: float = 1 / 12,\n",
    "    max_tempo: float = 250,\n",
    "    min_tempo: float = 30,\n",
    ") -> float:\n",
    "    # get note array\n",
    "    performance = pt.load_performance_midi(filename)\n",
    "    note_array = performance.note_array()\n",
    "    subbeats_per_beat = subbeats_from_durations(note_array)\n",
    "    tempi = tempi_from_iois(note_array, min_tempo, max_tempo)\n",
    "    if len(tempi) == 0:\n",
    "        tempi = \"auto\"\n",
    "\n",
    "    frames = get_frames_chordify(\n",
    "        note_array=note_array,\n",
    "        framerate=framerate,\n",
    "        chord_spread_time=chord_spread_time,\n",
    "        threshold=frame_threshold,\n",
    "    )\n",
    "\n",
    "    if tempi == \"auto\":\n",
    "        autocorr = compute_autocorrelation(frames)\n",
    "        beat_period, _ = find_peaks(autocorr[1:], prominence=None)\n",
    "        tempi = 60 * framerate / (beat_period + 1)\n",
    "        tempi = tempi[np.logical_and(tempi <= max_tempo, tempi >= min_tempo)]\n",
    "\n",
    "        if len(tempi) == 0:\n",
    "            tempi = np.linspace(min_tempo, max_tempo, 10)\n",
    "\n",
    "    likelihoods = []\n",
    "\n",
    "    for sbpb in subbeats_per_beat:\n",
    "        for tempo in tempi:\n",
    "            observation_model, transition_model = createHMM(\n",
    "                tempo=tempo,\n",
    "                frame_rate=framerate,\n",
    "                beats_per_measure=beats_per_measure,\n",
    "                subbeats_per_beat=sbpb,\n",
    "            )\n",
    "\n",
    "            hmm = HMM(\n",
    "                observation_model=observation_model,\n",
    "                transition_model=transition_model,\n",
    "            )\n",
    "\n",
    "            frames[frames < 1.0] = 0\n",
    "            frames[frames >= 1.0] = 1\n",
    "\n",
    "            observations = np.array(frames, dtype=int)\n",
    "            _, log_lik = hmm.find_best_sequence(observations)\n",
    "\n",
    "            likelihoods.append((sbpb, tempo, log_lik))\n",
    "\n",
    "    likelihoods = np.array(likelihoods)\n",
    "\n",
    "    best_result = likelihoods[likelihoods[:, 2].argmax()]\n",
    "\n",
    "    best_tempo = best_result[1]\n",
    "\n",
    "    return best_tempo\n",
    "\n",
    "def process_file(\n",
    "    mfn: PathLike, file_to_fix: Dict[str, Tuple[float, float]],\n",
    ") -> Tuple[str, int, float]:\n",
    "    piece: str = os.path.basename(mfn)\n",
    "    meter = int(file_to_fix[piece][0])\n",
    "    predicted_tempo: float = estimate_tempo(filename=mfn, beats_per_measure=meter)\n",
    "\n",
    "    return (\n",
    "        piece,\n",
    "        meter,\n",
    "        predicted_tempo,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:03:06.507698600Z",
     "start_time": "2023-12-27T14:03:06.480691100Z"
    }
   },
   "id": "a2bcddaf0f04a2c3"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def load_submission(fn: str) -> dict:\n",
    "    gt = np.loadtxt(\n",
    "        fn,\n",
    "        dtype=str,\n",
    "        delimiter=\",\",\n",
    "        comments=\"//\",\n",
    "    )\n",
    "\n",
    "    if gt.shape[1] > 3:\n",
    "        submission = dict([(g[0], (int(g[2]), float(g[4]))) for g in gt])\n",
    "    else:\n",
    "        submission = dict([(g[0], (int(g[1]), float(g[2]))) for g in gt])\n",
    "\n",
    "    return submission"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:03:08.537927300Z",
     "start_time": "2023-12-27T14:03:08.508975900Z"
    }
   },
   "id": "12863d6a117f2bfe"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": ".\\Dataset\\result.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[58], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m midi_files \u001B[38;5;241m=\u001B[39m glob\u001B[38;5;241m.\u001B[39mglob(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(args\u001B[38;5;241m.\u001B[39mdatadir, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*.mid\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     15\u001B[0m midi_files\u001B[38;5;241m.\u001B[39msort()\n\u001B[1;32m---> 17\u001B[0m file_to_fix \u001B[38;5;241m=\u001B[39m \u001B[43mload_submission\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Parallel processing with concurrent.futures\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ProcessPoolExecutor() \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# Using executor.map for parallel processing\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[55], line 2\u001B[0m, in \u001B[0;36mload_submission\u001B[1;34m(fn)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_submission\u001B[39m(fn: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n\u001B[1;32m----> 2\u001B[0m     gt \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloadtxt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomments\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m//\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gt\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m     10\u001B[0m         submission \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m([(g[\u001B[38;5;241m0\u001B[39m], (\u001B[38;5;28mint\u001B[39m(g[\u001B[38;5;241m2\u001B[39m]), \u001B[38;5;28mfloat\u001B[39m(g[\u001B[38;5;241m4\u001B[39m]))) \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m gt])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001B[0m, in \u001B[0;36mloadtxt\u001B[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001B[0m\n\u001B[0;32m   1370\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(delimiter, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m   1371\u001B[0m     delimiter \u001B[38;5;241m=\u001B[39m delimiter\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1373\u001B[0m arr \u001B[38;5;241m=\u001B[39m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdelimiter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1374\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconverters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconverters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskiplines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskiprows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musecols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43musecols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1375\u001B[0m \u001B[43m            \u001B[49m\u001B[43munpack\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munpack\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mndmin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1376\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_rows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\numpy\\lib\\npyio.py:992\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001B[0m\n\u001B[0;32m    990\u001B[0m     fname \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mfspath(fname)\n\u001B[0;32m    991\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fname, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 992\u001B[0m     fh \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_datasource\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    993\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m encoding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    994\u001B[0m         encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fh, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(path, mode, destpath, encoding, newline)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03mOpen `path` with `mode` and return the file object.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    189\u001B[0m \n\u001B[0;32m    190\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    192\u001B[0m ds \u001B[38;5;241m=\u001B[39m DataSource(destpath)\n\u001B[1;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnewline\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pm2s\\lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001B[0m, in \u001B[0;36mDataSource.open\u001B[1;34m(self, path, mode, encoding, newline)\u001B[0m\n\u001B[0;32m    530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _file_openers[ext](found, mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[0;32m    531\u001B[0m                               encoding\u001B[38;5;241m=\u001B[39mencoding, newline\u001B[38;5;241m=\u001B[39mnewline)\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 533\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: .\\Dataset\\result.txt not found."
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TempoEstimationArgs:\n",
    "    datadir = os.path.join('.', 'Dataset') # insert path to your dataset here\n",
    "    outfile = os.path.join('.', 'Dataset', 'result.txt') # insert path to your out file with meters (and dummy tempos) here\n",
    "\n",
    "\n",
    "args = TempoEstimationArgs()\n",
    "\n",
    "\n",
    "# Adapt this part as needed!\n",
    "midi_files = glob.glob(os.path.join(args.datadir, \"*.mid\"))\n",
    "midi_files.sort()\n",
    "\n",
    "file_to_fix = load_submission(args.outfile)\n",
    "\n",
    "# Parallel processing with concurrent.futures\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # Using executor.map for parallel processing\n",
    "    results_ = list(\n",
    "        tqdm(\n",
    "            executor.map(\n",
    "                process_file,\n",
    "                midi_files,\n",
    "                len(midi_files) * [file_to_fix],\n",
    "            ),\n",
    "            total=len(midi_files),\n",
    "        )\n",
    "    )\n",
    "\n",
    "results = [res[:3] for res in results_]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:03:56.711468100Z",
     "start_time": "2023-12-27T14:03:56.492055Z"
    }
   },
   "id": "5ec74a69c834a8d2"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a49baf21b5eb7309"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Calculating the tempo in this manner scored us a tempo error of just 9.691 with the test set, which ranks us 2nd on the leaderboard. Combined with the meter accuracy of 0.572, we are more than happy with our results compared to the others we see on the scoreboard. The tempo estimation can surely be enhanced further, either by improving the HMM or using another approach altogether, like an RNN again, but for this instance we think that our output is sufficient given the simplicity of its calculation as well as its dependence on the (mediocre) accuracy of the meter numerator."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "737b8ae65102f364"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "95de18b34eff8ef7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
